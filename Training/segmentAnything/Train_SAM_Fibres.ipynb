{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJJQ0HvPbkLt"
      },
      "source": [
        "## Training/Fine Tuning Segment Anything Model\n",
        "\n",
        "Based on Code from https://encord.com/blog/learn-how-to-fine-tune-the-segment-anything-model-sam/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1HKDMe7bkLu"
      },
      "source": [
        "### Link Colab to Drive and also import any relevant libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEI227lsbkLv"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "current_directory = '/content/drive/MyDrive/Colab Notebooks/FibreAnalysis/Data'\n",
        "\n",
        "from google.colab.patches import cv2_imshow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YacDZRbbkLv"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "from statistics import mean\n",
        "from tqdm import tqdm\n",
        "from torch.nn.functional import threshold, normalize\n",
        "from segment_anything import SamPredictor, sam_model_registry, SamAutomaticMaskGenerator\n",
        "import torch\n",
        "from matplotlib.patches import Rectangle\n",
        "from collections import defaultdict\n",
        "import torch\n",
        "from segment_anything.utils.transforms import ResizeLongestSide\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGkUKjTIbkLw"
      },
      "outputs": [],
      "source": [
        "current_directory = os.getcwd()\n",
        "OutPreparedImages = os.path.join(current_directory, 'Data', 'Prepared', 'Set', 'images', 'Train', '')\n",
        "OutPreparedMasks = os.path.join(current_directory, 'Data', 'Prepared', 'Set', 'masks', 'Train', '')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MndWwgiUbkLw"
      },
      "outputs": [],
      "source": [
        "# Helper functions provided in https://github.com/facebookresearch/segment-anything/blob/9e8f1309c94f1128a6e5c047a10fdcb02fc8d651/notebooks/predictor_example.ipynb\n",
        "\n",
        "def show_anns(anns): # From https://github.com/facebookresearch/segment-anything/blob/main/notebooks/predictor_example.ipynb\n",
        "    if len(anns) == 0:\n",
        "        return\n",
        "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
        "    ax = plt.gca()\n",
        "    ax.set_autoscale_on(False)\n",
        "\n",
        "    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n",
        "    img[:,:,3] = 0\n",
        "    for ann in sorted_anns:\n",
        "        m = ann['segmentation']\n",
        "        color_mask = np.concatenate([np.random.random(3), [0.35]])\n",
        "        img[m] = color_mask\n",
        "    ax.imshow(img)\n",
        "\n",
        "\n",
        "def show_masks(masks, ax, random_color=False):\n",
        "    for mask in masks:\n",
        "        if random_color:\n",
        "            color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "        else:\n",
        "            color = np.array([30/255, 144/255, 255/255, 0.6])\n",
        "        h, w = mask.shape[-2:]\n",
        "        mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "        ax.imshow(mask_image)\n",
        "\n",
        "def show_mask(mask, ax, random_color=False):\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "    else:\n",
        "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    ax.imshow(mask_image)\n",
        "\n",
        "def show_box(box, ax):\n",
        "    x0, y0 = box[0], box[1]\n",
        "    w, h = box[2] - box[0], box[3] - box[1]\n",
        "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))\n",
        "\n",
        "def show_boxes(bboxes, ax):\n",
        "    for bbox in bboxes:\n",
        "        x_min, y_min, x_max, y_max = bbox\n",
        "        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min,\n",
        "                             fill=False, edgecolor='red', linewidth=2)\n",
        "        ax.add_patch(rect)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgrcArkrbkLw"
      },
      "source": [
        "### Prepare Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niFpe50kbkLx"
      },
      "outputs": [],
      "source": [
        "# Model Checkpoints can be downloaded from https://github.com/facebookresearch/segment-anything#model-checkpoints\n",
        "# Options are base, Large and Huge Vit_b, Vit_L and Vit_H\n",
        "\n",
        "# Perhaps use a bigger model here????\n",
        "model_type = 'vit_b'\n",
        "checkpoint = r'/content/drive/MyDrive/Colab Notebooks/FibreAnalysis/Data/sam_vit_b_01ec64.pth'\n",
        "\n",
        "   # train on the GPU or on the CPU, if a GPU is not available - CPU will be very slow\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "sam_model = sam_model_registry[model_type](checkpoint=checkpoint)\n",
        "sam_model.to(device)\n",
        "sam_model.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4T3anMubkLx"
      },
      "source": [
        "### Build ground truth bounding boxes and object masks from the image masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GLmDJrbbkLx"
      },
      "outputs": [],
      "source": [
        "\n",
        "bbox_coords = {}\n",
        "ground_truth_masks = {}\n",
        "for f in sorted(Path(OutPreparedMasks).iterdir())[:100]:\n",
        "    k = f.stem[:]\n",
        "\n",
        "    mask = cv2.imread(f.as_posix(), cv2.IMREAD_GRAYSCALE)\n",
        "    _, mask1 = cv2.threshold(mask, 1, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    H, W = mask1.shape\n",
        "    contours, hierarchy = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    bounding_boxes = []\n",
        "    for contour in contours:\n",
        "        x, y, w, h = cv2.boundingRect(contour)\n",
        "        height, width = mask.shape\n",
        "        bounding_boxes.append(np.array([x, y, x + w, y + h]))\n",
        "    if len(bounding_boxes) > 0:\n",
        "        bbox_coords[k] = bounding_boxes\n",
        "\n",
        "    ground_truth_masks[k] = [~(mask == 0)] * len(bbox_coords[k])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukR7_6gkbkLx"
      },
      "outputs": [],
      "source": [
        "############################################################################################\n",
        "# Preprocess the images for use in the SAM Model\n",
        "############################################################################################\n",
        "\n",
        "\n",
        "transformed_data = defaultdict(dict)\n",
        "for k in bbox_coords.keys():\n",
        "\n",
        "  image_path = os.path.join(OutPreparedImages, f'image{k[4:]}.png')\n",
        "  image = cv2.imread(image_path)\n",
        "  #image = cv2.imread(f'{OutPreparedImages}{k}.png')\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "  transform = ResizeLongestSide(sam_model.image_encoder.img_size)\n",
        "  input_image = transform.apply_image(image)\n",
        "  input_image_torch = torch.as_tensor(input_image, device=device)\n",
        "  transformed_image = input_image_torch.permute(2, 0, 1).contiguous()[None, :, :, :]\n",
        "\n",
        "\n",
        "  input_image = sam_model.preprocess(transformed_image)\n",
        "  original_image_size = image.shape[:2]\n",
        "  input_size = tuple(transformed_image.shape[-2:])\n",
        "\n",
        "  transformed_data[k]['image'] = input_image\n",
        "  transformed_data[k]['input_size'] = input_size\n",
        "  transformed_data[k]['original_image_size'] = original_image_size\n",
        "\n",
        "############################################################################################\n",
        "\n",
        "############################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1v1818F-bkLx"
      },
      "source": [
        "### Set up hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSANi_PYbkLy"
      },
      "outputs": [],
      "source": [
        "# Set up the optimizer, hyperparameter tuning will improve performance here\n",
        "lr = 1e-4\n",
        "wd = 0\n",
        "optimizer = torch.optim.Adam(sam_model.mask_decoder.parameters(), lr=lr, weight_decay=wd)\n",
        "\n",
        "# loss_fn = torch.nn.MSELoss()\n",
        "# loss_fn = torch.nn.BCELoss()\n",
        "loss_fn = loss_fn = torch.nn.BCEWithLogitsLoss()  # More suitable loss function for multi mask pixel level similarities\n",
        "\n",
        "keys = list(set(bbox_coords.keys()))   # Get unique list of keys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uS4Yt8dbkLy"
      },
      "source": [
        "## Fine Tune the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CU9rbzQbkLy"
      },
      "outputs": [],
      "source": [
        "\n",
        "num_epochs = 2\n",
        "losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  epoch_losses = []\n",
        "  # Just train on the first 2 examples\n",
        "  for k in keys:\n",
        "    input_image = transformed_data[k]['image'].to(device)\n",
        "    input_size = transformed_data[k]['input_size']\n",
        "    original_image_size = transformed_data[k]['original_image_size']\n",
        "\n",
        "    # No grad here as we don't want to optimize the encoders\n",
        "    with torch.no_grad():\n",
        "      image_embedding = sam_model.image_encoder(input_image)\n",
        "\n",
        "      prompt_boxes = bbox_coords[k]  # Multiple bounding boxes\n",
        "      prompt_boxes1 = np.array(prompt_boxes)  # Convert to NumPy array\n",
        "      boxes = transform.apply_boxes(prompt_boxes1, original_image_size)\n",
        "\n",
        "######################################################################################\n",
        "\n",
        "######################################################################################\n",
        "# Convert the image tensor to a numpy array\n",
        "    #  image_np = input_image.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
        "\n",
        "\n",
        "      boxes_torch = torch.as_tensor(boxes, dtype=torch.float, device=device)\n",
        "\n",
        "      sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(\n",
        "          points=None,\n",
        "          boxes=boxes_torch,\n",
        "          masks=None,\n",
        "      )\n",
        "\n",
        "    low_res_masks, iou_predictions = sam_model.mask_decoder(\n",
        "      image_embeddings=image_embedding,\n",
        "      image_pe=sam_model.prompt_encoder.get_dense_pe(),\n",
        "      sparse_prompt_embeddings=sparse_embeddings,\n",
        "      dense_prompt_embeddings=dense_embeddings,\n",
        "      multimask_output=True,  # Output multiple masks\n",
        "    )\n",
        "\n",
        "    upscaled_masks = sam_model.postprocess_masks(low_res_masks, input_size, original_image_size).to(device)\n",
        "    # Convert RGB image to grayscale\n",
        "    gray_image = torch.mean(upscaled_masks, dim=1, keepdim=True)\n",
        "    binary_masks = normalize(threshold(gray_image, 0.0, 0))\n",
        "\n",
        "    gt_masks_resized = []\n",
        "    for gt_mask in ground_truth_masks[k]:  # Loop over multiple ground truth masks\n",
        "        gt_mask_resized = torch.from_numpy(np.resize(gt_mask, (1, gt_mask.shape[0], gt_mask.shape[1]))).to(device)\n",
        "\n",
        "        gt_binary_mask = torch.as_tensor(gt_mask_resized > 0, dtype=torch.float32)\n",
        "        gt_masks_resized.append(gt_binary_mask)\n",
        "\n",
        "# Stack the individual gt_binary_mask tensors along the batch dimension\n",
        "    gt_masks_tensor = torch.stack(gt_masks_resized, dim=0)\n",
        "\n",
        " #   loss = loss_fn(binary_masks, torch.stack(gt_masks_resized))\n",
        "    loss = loss_fn(binary_masks, gt_masks_tensor)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    epoch_losses.append(loss.item())\n",
        "\n",
        "  losses.append(epoch_losses)\n",
        "  print(f'EPOCH: {epoch}')\n",
        "  print(f'Loss: {mean(epoch_losses)}')\n",
        "\n",
        "PATH = r'/content/drive/MyDrive/Colab Notebooks/FibreAnalysis'\n",
        "torch.save(sam_model.state_dict(), PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnfuwzFebkLy"
      },
      "source": [
        "## Display losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bLJ5Gc_bkLy"
      },
      "outputs": [],
      "source": [
        "mean_losses = [mean(x) for x in losses]\n",
        "# mean_losses\n",
        "\n",
        "plt.plot(list(range(len(mean_losses))), mean_losses)\n",
        "plt.title('Mean epoch loss')\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3tsIA2YbkLy"
      },
      "source": [
        "### Display test image with the predicted masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQMes0jzbkLy"
      },
      "outputs": [],
      "source": [
        "\n",
        "img_files = os.listdir(os.path.join(current_directory, 'train/val')) # Get list of files in the directory\n",
        "for imageFile in random.sample(img_files, 1):  # Take one example\n",
        "\n",
        "   image = cv2.imread(imageFile)\n",
        "   mask_generator = SamAutomaticMaskGenerator(sam_model)\n",
        "   masks = mask_generator.generate(image)\n",
        "\n",
        "   plt.figure(figsize=(20,20))\n",
        "   plt.imshow(image)\n",
        "   show_anns(masks)\n",
        "   plt.axis('off')\n",
        "   plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}