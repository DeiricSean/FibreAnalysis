{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJJQ0HvPbkLt"
      },
      "source": [
        "## Training/Fine Tuning Segment Anything Model\n",
        "\n",
        "Based on Code from https://encord.com/blog/learn-how-to-fine-tune-the-segment-anything-model-sam/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1HKDMe7bkLu"
      },
      "source": [
        "### Link Colab to Drive and also import any relevant libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEI227lsbkLv",
        "outputId": "5cf21fe3-18c6-40e7-94aa-288a880dbea4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "current_directory = '/content/drive/MyDrive/Colab Notebooks/FibreAnalysis/Data'\n",
        "\n",
        "from google.colab.patches import cv2_imshow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "using_colab = True\n",
        "if using_colab:\n",
        "    import torch\n",
        "    import torchvision\n",
        "    print(\"PyTorch version:\", torch.__version__)\n",
        "    print(\"Torchvision version:\", torchvision.__version__)\n",
        "    print(\"CUDA is available:\", torch.cuda.is_available())\n",
        "    import sys\n",
        "    !{sys.executable} -m pip install opencv-python matplotlib\n",
        "    !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n",
        "\n",
        "    !mkdir images\n",
        "    !wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/dog.jpg\n",
        "\n",
        "    !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
      ],
      "metadata": {
        "id": "Wh4XYCVsdAFw",
        "outputId": "a2671136-db63-4646-95f1-e59423be9e26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.0.1+cu118\n",
            "Torchvision version: 0.15.2+cu118\n",
            "CUDA is available: True\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.7.0.72)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.22.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.41.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Collecting git+https://github.com/facebookresearch/segment-anything.git\n",
            "  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-req-build-ye35ckgo\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-req-build-ye35ckgo\n",
            "  Resolved https://github.com/facebookresearch/segment-anything.git to commit 6fdee8f2727f4506cfbbe553e23b895e27956588\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: segment-anything\n",
            "  Building wheel for segment-anything (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segment-anything: filename=segment_anything-1.0-py3-none-any.whl size=36588 sha256=171bdd9bc5bc539d35dd31876d734703005acb917bf507cde2b70e732868a4db\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-q_fc1l6s/wheels/10/cf/59/9ccb2f0a1bcc81d4fbd0e501680b5d088d690c6cfbc02dc99d\n",
            "Successfully built segment-anything\n",
            "Installing collected packages: segment-anything\n",
            "Successfully installed segment-anything-1.0\n",
            "--2023-07-27 08:26:20--  https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/dog.jpg\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 99846 (98K) [image/jpeg]\n",
            "Saving to: ‘images/dog.jpg’\n",
            "\n",
            "dog.jpg             100%[===================>]  97.51K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2023-07-27 08:26:20 (65.6 MB/s) - ‘images/dog.jpg’ saved [99846/99846]\n",
            "\n",
            "--2023-07-27 08:26:20--  https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.224.249.83, 13.224.249.94, 13.224.249.96, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.224.249.83|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2564550879 (2.4G) [binary/octet-stream]\n",
            "Saving to: ‘sam_vit_h_4b8939.pth’\n",
            "\n",
            "sam_vit_h_4b8939.pt 100%[===================>]   2.39G   196MB/s    in 12s     \n",
            "\n",
            "2023-07-27 08:26:33 (196 MB/s) - ‘sam_vit_h_4b8939.pth’ saved [2564550879/2564550879]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4YacDZRbbkLv"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "from statistics import mean\n",
        "from tqdm import tqdm\n",
        "from torch.nn.functional import threshold, normalize\n",
        "from segment_anything import SamPredictor, sam_model_registry, SamAutomaticMaskGenerator\n",
        "import torch\n",
        "from matplotlib.patches import Rectangle\n",
        "from collections import defaultdict\n",
        "import torch\n",
        "from segment_anything.utils.transforms import ResizeLongestSide\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dGkUKjTIbkLw"
      },
      "outputs": [],
      "source": [
        "current_directory = os.getcwd()\n",
        "OutPreparedImages = os.path.join(current_directory, 'Data', 'Prepared', 'Set', 'images', 'Train', '')\n",
        "OutPreparedMasks = os.path.join(current_directory, 'Data', 'Prepared', 'Set', 'masks', 'Train', '')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MndWwgiUbkLw"
      },
      "outputs": [],
      "source": [
        "# Helper functions provided in https://github.com/facebookresearch/segment-anything/blob/9e8f1309c94f1128a6e5c047a10fdcb02fc8d651/notebooks/predictor_example.ipynb\n",
        "\n",
        "def show_anns(anns): # From https://github.com/facebookresearch/segment-anything/blob/main/notebooks/predictor_example.ipynb\n",
        "    if len(anns) == 0:\n",
        "        return\n",
        "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
        "    ax = plt.gca()\n",
        "    ax.set_autoscale_on(False)\n",
        "\n",
        "    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n",
        "    img[:,:,3] = 0\n",
        "    for ann in sorted_anns:\n",
        "        m = ann['segmentation']\n",
        "        color_mask = np.concatenate([np.random.random(3), [0.35]])\n",
        "        img[m] = color_mask\n",
        "    ax.imshow(img)\n",
        "\n",
        "\n",
        "def show_masks(masks, ax, random_color=False):\n",
        "    for mask in masks:\n",
        "        if random_color:\n",
        "            color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "        else:\n",
        "            color = np.array([30/255, 144/255, 255/255, 0.6])\n",
        "        h, w = mask.shape[-2:]\n",
        "        mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "        ax.imshow(mask_image)\n",
        "\n",
        "def show_mask(mask, ax, random_color=False):\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "    else:\n",
        "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    ax.imshow(mask_image)\n",
        "\n",
        "def show_box(box, ax):\n",
        "    x0, y0 = box[0], box[1]\n",
        "    w, h = box[2] - box[0], box[3] - box[1]\n",
        "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))\n",
        "\n",
        "def show_boxes(bboxes, ax):\n",
        "    for bbox in bboxes:\n",
        "        x_min, y_min, x_max, y_max = bbox\n",
        "        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min,\n",
        "                             fill=False, edgecolor='red', linewidth=2)\n",
        "        ax.add_patch(rect)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgrcArkrbkLw"
      },
      "source": [
        "### Prepare Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niFpe50kbkLx"
      },
      "outputs": [],
      "source": [
        "# Model Checkpoints can be downloaded from https://github.com/facebookresearch/segment-anything#model-checkpoints\n",
        "# Options are base, Large and Huge Vit_b, Vit_L and Vit_H\n",
        "\n",
        "# Perhaps use a bigger model here????\n",
        "model_type = 'vit_b'\n",
        "checkpoint = r'/content/drive/MyDrive/Colab Notebooks/FibreAnalysis/Data/sam_vit_h_4b8939.pth'\n",
        "\n",
        "   # train on the GPU or on the CPU, if a GPU is not available - CPU will be very slow\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "sam_model = sam_model_registry[model_type](checkpoint=checkpoint)\n",
        "sam_model.to(device)\n",
        "sam_model.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4T3anMubkLx"
      },
      "source": [
        "### Build ground truth bounding boxes and object masks from the image masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GLmDJrbbkLx"
      },
      "outputs": [],
      "source": [
        "\n",
        "bbox_coords = {}\n",
        "ground_truth_masks = {}\n",
        "for f in sorted(Path(OutPreparedMasks).iterdir())[:100]:\n",
        "    k = f.stem[:]\n",
        "\n",
        "    mask = cv2.imread(f.as_posix(), cv2.IMREAD_GRAYSCALE)\n",
        "    _, mask1 = cv2.threshold(mask, 1, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    H, W = mask1.shape\n",
        "    contours, hierarchy = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    bounding_boxes = []\n",
        "    for contour in contours:\n",
        "        x, y, w, h = cv2.boundingRect(contour)\n",
        "        height, width = mask.shape\n",
        "        bounding_boxes.append(np.array([x, y, x + w, y + h]))\n",
        "    if len(bounding_boxes) > 0:\n",
        "        bbox_coords[k] = bounding_boxes\n",
        "\n",
        "    ground_truth_masks[k] = [~(mask == 0)] * len(bbox_coords[k])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukR7_6gkbkLx"
      },
      "outputs": [],
      "source": [
        "############################################################################################\n",
        "# Preprocess the images for use in the SAM Model\n",
        "############################################################################################\n",
        "\n",
        "\n",
        "transformed_data = defaultdict(dict)\n",
        "for k in bbox_coords.keys():\n",
        "\n",
        "  image_path = os.path.join(OutPreparedImages, f'image{k[4:]}.png')\n",
        "  image = cv2.imread(image_path)\n",
        "  #image = cv2.imread(f'{OutPreparedImages}{k}.png')\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "  transform = ResizeLongestSide(sam_model.image_encoder.img_size)\n",
        "  input_image = transform.apply_image(image)\n",
        "  input_image_torch = torch.as_tensor(input_image, device=device)\n",
        "  transformed_image = input_image_torch.permute(2, 0, 1).contiguous()[None, :, :, :]\n",
        "\n",
        "\n",
        "  input_image = sam_model.preprocess(transformed_image)\n",
        "  original_image_size = image.shape[:2]\n",
        "  input_size = tuple(transformed_image.shape[-2:])\n",
        "\n",
        "  transformed_data[k]['image'] = input_image\n",
        "  transformed_data[k]['input_size'] = input_size\n",
        "  transformed_data[k]['original_image_size'] = original_image_size\n",
        "\n",
        "############################################################################################\n",
        "\n",
        "############################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1v1818F-bkLx"
      },
      "source": [
        "### Set up hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSANi_PYbkLy"
      },
      "outputs": [],
      "source": [
        "# Set up the optimizer, hyperparameter tuning will improve performance here\n",
        "lr = 1e-4\n",
        "wd = 0\n",
        "optimizer = torch.optim.Adam(sam_model.mask_decoder.parameters(), lr=lr, weight_decay=wd)\n",
        "\n",
        "# loss_fn = torch.nn.MSELoss()\n",
        "# loss_fn = torch.nn.BCELoss()\n",
        "loss_fn = loss_fn = torch.nn.BCEWithLogitsLoss()  # More suitable loss function for multi mask pixel level similarities\n",
        "\n",
        "keys = list(set(bbox_coords.keys()))   # Get unique list of keys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uS4Yt8dbkLy"
      },
      "source": [
        "## Fine Tune the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CU9rbzQbkLy"
      },
      "outputs": [],
      "source": [
        "\n",
        "num_epochs = 2\n",
        "losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  epoch_losses = []\n",
        "  # Just train on the first 2 examples\n",
        "  for k in keys:\n",
        "    input_image = transformed_data[k]['image'].to(device)\n",
        "    input_size = transformed_data[k]['input_size']\n",
        "    original_image_size = transformed_data[k]['original_image_size']\n",
        "\n",
        "    # No grad here as we don't want to optimize the encoders\n",
        "    with torch.no_grad():\n",
        "      image_embedding = sam_model.image_encoder(input_image)\n",
        "\n",
        "      prompt_boxes = bbox_coords[k]  # Multiple bounding boxes\n",
        "      prompt_boxes1 = np.array(prompt_boxes)  # Convert to NumPy array\n",
        "      boxes = transform.apply_boxes(prompt_boxes1, original_image_size)\n",
        "\n",
        "######################################################################################\n",
        "\n",
        "######################################################################################\n",
        "# Convert the image tensor to a numpy array\n",
        "    #  image_np = input_image.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
        "\n",
        "\n",
        "      boxes_torch = torch.as_tensor(boxes, dtype=torch.float, device=device)\n",
        "\n",
        "      sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(\n",
        "          points=None,\n",
        "          boxes=boxes_torch,\n",
        "          masks=None,\n",
        "      )\n",
        "\n",
        "    low_res_masks, iou_predictions = sam_model.mask_decoder(\n",
        "      image_embeddings=image_embedding,\n",
        "      image_pe=sam_model.prompt_encoder.get_dense_pe(),\n",
        "      sparse_prompt_embeddings=sparse_embeddings,\n",
        "      dense_prompt_embeddings=dense_embeddings,\n",
        "      multimask_output=True,  # Output multiple masks\n",
        "    )\n",
        "\n",
        "    upscaled_masks = sam_model.postprocess_masks(low_res_masks, input_size, original_image_size).to(device)\n",
        "    # Convert RGB image to grayscale\n",
        "    gray_image = torch.mean(upscaled_masks, dim=1, keepdim=True)\n",
        "    binary_masks = normalize(threshold(gray_image, 0.0, 0))\n",
        "\n",
        "    gt_masks_resized = []\n",
        "    for gt_mask in ground_truth_masks[k]:  # Loop over multiple ground truth masks\n",
        "        gt_mask_resized = torch.from_numpy(np.resize(gt_mask, (1, gt_mask.shape[0], gt_mask.shape[1]))).to(device)\n",
        "\n",
        "        gt_binary_mask = torch.as_tensor(gt_mask_resized > 0, dtype=torch.float32)\n",
        "        gt_masks_resized.append(gt_binary_mask)\n",
        "\n",
        "# Stack the individual gt_binary_mask tensors along the batch dimension\n",
        "    gt_masks_tensor = torch.stack(gt_masks_resized, dim=0)\n",
        "\n",
        " #   loss = loss_fn(binary_masks, torch.stack(gt_masks_resized))\n",
        "    loss = loss_fn(binary_masks, gt_masks_tensor)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    epoch_losses.append(loss.item())\n",
        "\n",
        "  losses.append(epoch_losses)\n",
        "  print(f'EPOCH: {epoch}')\n",
        "  print(f'Loss: {mean(epoch_losses)}')\n",
        "\n",
        "PATH = r'/content/drive/MyDrive/Colab Notebooks/FibreAnalysis'\n",
        "torch.save(sam_model.state_dict(), PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnfuwzFebkLy"
      },
      "source": [
        "## Display losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bLJ5Gc_bkLy"
      },
      "outputs": [],
      "source": [
        "mean_losses = [mean(x) for x in losses]\n",
        "# mean_losses\n",
        "\n",
        "plt.plot(list(range(len(mean_losses))), mean_losses)\n",
        "plt.title('Mean epoch loss')\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3tsIA2YbkLy"
      },
      "source": [
        "### Display test image with the predicted masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQMes0jzbkLy"
      },
      "outputs": [],
      "source": [
        "\n",
        "img_files = os.listdir(os.path.join(current_directory, 'train/val')) # Get list of files in the directory\n",
        "for imageFile in random.sample(img_files, 1):  # Take one example\n",
        "\n",
        "   image = cv2.imread(imageFile)\n",
        "   mask_generator = SamAutomaticMaskGenerator(sam_model)\n",
        "   masks = mask_generator.generate(image)\n",
        "\n",
        "   plt.figure(figsize=(20,20))\n",
        "   plt.imshow(image)\n",
        "   show_anns(masks)\n",
        "   plt.axis('off')\n",
        "   plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}