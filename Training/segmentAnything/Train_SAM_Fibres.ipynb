{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJJQ0HvPbkLt"
      },
      "source": [
        "## Training/Fine Tuning Segment Anything Model\n",
        "\n",
        "Based on Code from https://encord.com/blog/learn-how-to-fine-tune-the-segment-anything-model-sam/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1HKDMe7bkLu"
      },
      "source": [
        "### Link Colab to Drive and also import any relevant libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEI227lsbkLv",
        "outputId": "75e138e6-19e2-4398-93f2-2a0def0c9602"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "current_directory = '/content/drive/MyDrive/Colab Notebooks/FibreAnalysis/Data'\n",
        "\n",
        "from google.colab.patches import cv2_imshow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "using_colab = True\n",
        "if using_colab:\n",
        "    import torch\n",
        "    import torchvision\n",
        "    print(\"PyTorch version:\", torch.__version__)\n",
        "    print(\"Torchvision version:\", torchvision.__version__)\n",
        "    print(\"CUDA is available:\", torch.cuda.is_available())\n",
        "    import sys\n",
        "    !{sys.executable} -m pip install opencv-python matplotlib\n",
        "    !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n",
        "\n",
        "    !mkdir images\n",
        "    !wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/dog.jpg\n",
        "\n",
        "    !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wh4XYCVsdAFw",
        "outputId": "15fef608-be13-4453-c2bc-9224a8b9b594"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.0.1+cu118\n",
            "Torchvision version: 0.15.2+cu118\n",
            "CUDA is available: True\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.7.0.72)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.22.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.41.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Collecting git+https://github.com/facebookresearch/segment-anything.git\n",
            "  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-req-build-qayi155l\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-req-build-qayi155l\n",
            "  Resolved https://github.com/facebookresearch/segment-anything.git to commit 6fdee8f2727f4506cfbbe553e23b895e27956588\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: segment-anything\n",
            "  Building wheel for segment-anything (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segment-anything: filename=segment_anything-1.0-py3-none-any.whl size=36588 sha256=82f4903ad9a86259d8a5b340e22942495f7213cfa5a40785d06b657c9a6c8597\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-vz5zkoe5/wheels/10/cf/59/9ccb2f0a1bcc81d4fbd0e501680b5d088d690c6cfbc02dc99d\n",
            "Successfully built segment-anything\n",
            "Installing collected packages: segment-anything\n",
            "Successfully installed segment-anything-1.0\n",
            "--2023-07-31 15:13:18--  https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/dog.jpg\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 99846 (98K) [image/jpeg]\n",
            "Saving to: ‘images/dog.jpg’\n",
            "\n",
            "dog.jpg             100%[===================>]  97.51K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-07-31 15:13:18 (6.78 MB/s) - ‘images/dog.jpg’ saved [99846/99846]\n",
            "\n",
            "--2023-07-31 15:13:18--  https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 65.8.248.107, 65.8.248.22, 65.8.248.127, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|65.8.248.107|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2564550879 (2.4G) [binary/octet-stream]\n",
            "Saving to: ‘sam_vit_h_4b8939.pth’\n",
            "\n",
            "sam_vit_h_4b8939.pt 100%[===================>]   2.39G   151MB/s    in 17s     \n",
            "\n",
            "2023-07-31 15:13:35 (146 MB/s) - ‘sam_vit_h_4b8939.pth’ saved [2564550879/2564550879]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4YacDZRbbkLv"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "from statistics import mean\n",
        "from tqdm import tqdm\n",
        "from torch.nn.functional import threshold, normalize\n",
        "from segment_anything import SamPredictor, sam_model_registry, SamAutomaticMaskGenerator\n",
        "import torch\n",
        "from matplotlib.patches import Rectangle\n",
        "from collections import defaultdict\n",
        "import torch\n",
        "from segment_anything.utils.transforms import ResizeLongestSide\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dGkUKjTIbkLw"
      },
      "outputs": [],
      "source": [
        "#current_directory = os.getcwd()\n",
        "current_directory = r'/content/drive/MyDrive/Colab Notebooks/FibreAnalysis'\n",
        "OutPreparedImages = os.path.join(current_directory, 'Data', 'Prepared', 'Set', 'images', 'train', '')\n",
        "OutPreparedMasks = os.path.join(current_directory, 'Data', 'Prepared', 'Set', 'masks', 'train', '')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MndWwgiUbkLw"
      },
      "outputs": [],
      "source": [
        "# Helper functions provided in https://github.com/facebookresearch/segment-anything/blob/9e8f1309c94f1128a6e5c047a10fdcb02fc8d651/notebooks/predictor_example.ipynb\n",
        "\n",
        "def show_anns(anns): # From https://github.com/facebookresearch/segment-anything/blob/main/notebooks/predictor_example.ipynb\n",
        "    if len(anns) == 0:\n",
        "        return\n",
        "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
        "    ax = plt.gca()\n",
        "    ax.set_autoscale_on(False)\n",
        "\n",
        "    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n",
        "    img[:,:,3] = 0\n",
        "    for ann in sorted_anns:\n",
        "        m = ann['segmentation']\n",
        "        color_mask = np.concatenate([np.random.random(3), [0.35]])\n",
        "        img[m] = color_mask\n",
        "    ax.imshow(img)\n",
        "\n",
        "\n",
        "def show_masks(masks, ax, random_color=False):\n",
        "    for mask in masks:\n",
        "        if random_color:\n",
        "            color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "        else:\n",
        "            color = np.array([30/255, 144/255, 255/255, 0.6])\n",
        "        h, w = mask.shape[-2:]\n",
        "        mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "        ax.imshow(mask_image)\n",
        "\n",
        "def show_mask(mask, ax, random_color=False):\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "    else:\n",
        "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    ax.imshow(mask_image)\n",
        "\n",
        "def show_box(box, ax):\n",
        "    x0, y0 = box[0], box[1]\n",
        "    w, h = box[2] - box[0], box[3] - box[1]\n",
        "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))\n",
        "\n",
        "def show_boxes(bboxes, ax):\n",
        "    for bbox in bboxes:\n",
        "        x_min, y_min, x_max, y_max = bbox\n",
        "        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min,\n",
        "                             fill=False, edgecolor='red', linewidth=2)\n",
        "        ax.add_patch(rect)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgrcArkrbkLw"
      },
      "source": [
        "### Prepare Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niFpe50kbkLx",
        "outputId": "ae14e35f-2c42-42fd-dab1-9dbc2d50cde7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sam(\n",
              "  (image_encoder): ImageEncoderViT(\n",
              "    (patch_embed): PatchEmbed(\n",
              "      (proj): Conv2d(3, 1280, kernel_size=(16, 16), stride=(16, 16))\n",
              "    )\n",
              "    (blocks): ModuleList(\n",
              "      (0-31): 32 x Block(\n",
              "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
              "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (act): GELU(approximate='none')\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (neck): Sequential(\n",
              "      (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (1): LayerNorm2d()\n",
              "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (3): LayerNorm2d()\n",
              "    )\n",
              "  )\n",
              "  (prompt_encoder): PromptEncoder(\n",
              "    (pe_layer): PositionEmbeddingRandom()\n",
              "    (point_embeddings): ModuleList(\n",
              "      (0-3): 4 x Embedding(1, 256)\n",
              "    )\n",
              "    (not_a_point_embed): Embedding(1, 256)\n",
              "    (mask_downscaling): Sequential(\n",
              "      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (1): LayerNorm2d()\n",
              "      (2): GELU(approximate='none')\n",
              "      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (4): LayerNorm2d()\n",
              "      (5): GELU(approximate='none')\n",
              "      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "    (no_mask_embed): Embedding(1, 256)\n",
              "  )\n",
              "  (mask_decoder): MaskDecoder(\n",
              "    (transformer): TwoWayTransformer(\n",
              "      (layers): ModuleList(\n",
              "        (0-1): 2 x TwoWayAttentionBlock(\n",
              "          (self_attn): Attention(\n",
              "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "          )\n",
              "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (cross_attn_token_to_image): Attention(\n",
              "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
              "          )\n",
              "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): MLPBlock(\n",
              "            (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
              "            (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
              "            (act): ReLU()\n",
              "          )\n",
              "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (cross_attn_image_to_token): Attention(\n",
              "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (final_attn_token_to_image): Attention(\n",
              "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
              "      )\n",
              "      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (iou_token): Embedding(1, 256)\n",
              "    (mask_tokens): Embedding(4, 256)\n",
              "    (output_upscaling): Sequential(\n",
              "      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (1): LayerNorm2d()\n",
              "      (2): GELU(approximate='none')\n",
              "      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (4): GELU(approximate='none')\n",
              "    )\n",
              "    (output_hypernetworks_mlps): ModuleList(\n",
              "      (0-3): 4 x MLP(\n",
              "        (layers): ModuleList(\n",
              "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
              "          (2): Linear(in_features=256, out_features=32, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (iou_prediction_head): MLP(\n",
              "      (layers): ModuleList(\n",
              "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
              "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# Model Checkpoints can be downloaded from https://github.com/facebookresearch/segment-anything#model-checkpoints\n",
        "# Options are base, Large and Huge Vit_b, Vit_L and Vit_H\n",
        "\n",
        "# Perhaps use a bigger model here????\n",
        "model_type = 'vit_h'\n",
        "#checkpoint = r'/content/drive/MyDrive/Colab Notebooks/FibreAnalysis/Data/sam_vit_h_4b8939.pth'\n",
        "checkpoint = r'/content/sam_vit_h_4b8939.pth'\n",
        "   # train on the GPU or on the CPU, if a GPU is not available - CPU will be very slow\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "sam_model = sam_model_registry[model_type](checkpoint=checkpoint)\n",
        "sam_model.to(device)\n",
        "sam_model.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4T3anMubkLx"
      },
      "source": [
        "### Build ground truth bounding boxes and object masks from the image masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9GLmDJrbbkLx"
      },
      "outputs": [],
      "source": [
        "\n",
        "bbox_coords = {}\n",
        "ground_truth_masks = {}\n",
        "#for f in sorted(Path(OutPreparedMasks).iterdir()):\n",
        "for f in random.sample(sorted(Path(OutPreparedMasks).iterdir()), 100):\n",
        "    k = f.stem[:]\n",
        "\n",
        "    mask = cv2.imread(f.as_posix(), cv2.IMREAD_GRAYSCALE)\n",
        "    _, mask1 = cv2.threshold(mask, 1, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    H, W = mask1.shape\n",
        "    contours, hierarchy = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    bounding_boxes = []\n",
        "    for contour in contours:\n",
        "        x, y, w, h = cv2.boundingRect(contour)\n",
        "        height, width = mask.shape\n",
        "        bounding_boxes.append(np.array([x, y, x + w, y + h]))\n",
        "    if len(bounding_boxes) > 0:\n",
        "        bbox_coords[k] = bounding_boxes\n",
        "\n",
        "        ground_truth_masks[k] = [~(mask == 0)] * len(bbox_coords[k])\n",
        "\n",
        "    # # Assuming 'bbox_coords' is a dictionary with bounding box coordinates for each key 'k'\n",
        "    # for k, bbox_list in bbox_coords[k].items():\n",
        "    #     masks_for_k = []\n",
        "    #     for bbox_coord in bbox_list:\n",
        "    #         # Assuming 'mask' is a 2D NumPy array or a PyTorch tensor\n",
        "    #         mask_for_bbox = ~(mask == 0)\n",
        "    #         masks_for_k.append(mask_for_bbox)\n",
        "    #     ground_truth_masks[k] = masks_for_k\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ukR7_6gkbkLx"
      },
      "outputs": [],
      "source": [
        "############################################################################################\n",
        "# Preprocess the images for use in the SAM Model\n",
        "############################################################################################\n",
        "\n",
        "\n",
        "transformed_data = defaultdict(dict)\n",
        "for k in bbox_coords.keys():\n",
        "\n",
        "  image_path = os.path.join(OutPreparedImages, f'image{k[4:]}.png')\n",
        "  image = cv2.imread(image_path)\n",
        "  #image = cv2.imread(f'{OutPreparedImages}{k}.png')\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "  transform = ResizeLongestSide(sam_model.image_encoder.img_size)\n",
        "  input_image = transform.apply_image(image)\n",
        "  input_image_torch = torch.as_tensor(input_image, device=device)\n",
        "  transformed_image = input_image_torch.permute(2, 0, 1).contiguous()[None, :, :, :]\n",
        "\n",
        "\n",
        "  input_image = sam_model.preprocess(transformed_image)\n",
        "  original_image_size = image.shape[:2]\n",
        "  input_size = tuple(transformed_image.shape[-2:])\n",
        "\n",
        "  transformed_data[k]['image'] = input_image\n",
        "  transformed_data[k]['input_size'] = input_size\n",
        "  transformed_data[k]['original_image_size'] = original_image_size\n",
        "\n",
        "############################################################################################\n",
        "\n",
        "############################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1v1818F-bkLx"
      },
      "source": [
        "### Set up hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "BSANi_PYbkLy"
      },
      "outputs": [],
      "source": [
        "# Set up the optimizer, hyperparameter tuning will improve performance here\n",
        "lr = 1e-2\n",
        "wd = 0\n",
        "optimizer = torch.optim.Adam(sam_model.mask_decoder.parameters(), lr=lr, weight_decay=wd)\n",
        "\n",
        "# loss_fn = torch.nn.MSELoss()\n",
        "# loss_fn = torch.nn.BCELoss()\n",
        "loss_fn = loss_fn = torch.nn.BCEWithLogitsLoss()  # More suitable loss function for multi mask pixel level similarities\n",
        "\n",
        "keys = list(set(bbox_coords.keys()))   # Get unique list of keys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uS4Yt8dbkLy"
      },
      "source": [
        "## Fine Tune the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "1CU9rbzQbkLy",
        "outputId": "8961d8bf-3ff1-4b1a-cc58-6e6024c42ea3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-26b665274395>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# No grad here as we don't want to optimize the encoders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m       \u001b[0mimage_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msam_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0mprompt_boxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbbox_coords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Multiple bounding boxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/segment_anything/modeling/image_encoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_embed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_embed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/segment_anything/modeling/image_encoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m         \u001b[0;31m# B C H W -> B H W C\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR"
          ]
        }
      ],
      "source": [
        "\n",
        "num_epochs = 100\n",
        "losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  epoch_losses = []\n",
        "  # Just train on the first 2 examples\n",
        "  #for k in keys:\n",
        "  for k in random.sample(keys, 10):\n",
        "    input_image = transformed_data[k]['image'].to(device)\n",
        "    input_size = transformed_data[k]['input_size']\n",
        "    original_image_size = transformed_data[k]['original_image_size']\n",
        "\n",
        "    # No grad here as we don't want to optimize the encoders\n",
        "    with torch.no_grad():\n",
        "      image_embedding = sam_model.image_encoder(input_image)\n",
        "\n",
        "      prompt_boxes = bbox_coords[k]  # Multiple bounding boxes\n",
        "      prompt_boxes1 = np.array(prompt_boxes)  # Convert to NumPy array\n",
        "      boxes = transform.apply_boxes(prompt_boxes1, original_image_size)\n",
        "\n",
        "######################################################################################\n",
        "\n",
        "######################################################################################\n",
        "# Convert the image tensor to a numpy array\n",
        "    #  image_np = input_image.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
        "\n",
        "\n",
        "      boxes_torch = torch.as_tensor(boxes, dtype=torch.float, device=device)\n",
        "\n",
        "      sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(\n",
        "          points=None,\n",
        "          boxes=boxes_torch,\n",
        "          masks=None,\n",
        "      )\n",
        "\n",
        "    low_res_masks, iou_predictions = sam_model.mask_decoder(\n",
        "      image_embeddings=image_embedding,\n",
        "      image_pe=sam_model.prompt_encoder.get_dense_pe(),\n",
        "      sparse_prompt_embeddings=sparse_embeddings,\n",
        "      dense_prompt_embeddings=dense_embeddings,\n",
        "      multimask_output=True,  # Output multiple masks\n",
        "    )\n",
        "\n",
        "    upscaled_masks = sam_model.postprocess_masks(low_res_masks, input_size, original_image_size).to(device)\n",
        "    # Convert RGB image to grayscale\n",
        "    gray_image = torch.mean(upscaled_masks, dim=1, keepdim=True)\n",
        "    binary_masks = normalize(threshold(gray_image, 0.0, 0))\n",
        "\n",
        "    gt_masks_resized = []\n",
        "    for gt_mask in ground_truth_masks[k]:  # Loop over multiple ground truth masks\n",
        "        gt_mask_resized = torch.from_numpy(np.resize(gt_mask, (1, gt_mask.shape[0], gt_mask.shape[1]))).to(device)\n",
        "\n",
        "        gt_binary_mask = torch.as_tensor(gt_mask_resized > 0, dtype=torch.float32)\n",
        "        gt_masks_resized.append(gt_binary_mask)\n",
        "\n",
        "# Stack the individual gt_binary_mask tensors along the batch dimension\n",
        "    gt_masks_tensor = torch.stack(gt_masks_resized, dim=0)\n",
        "\n",
        " #   loss = loss_fn(binary_masks, torch.stack(gt_masks_resized))\n",
        "    loss = loss_fn(binary_masks, gt_masks_tensor)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    epoch_losses.append(loss.item())\n",
        "\n",
        "  losses.append(epoch_losses)\n",
        "  print(f'EPOCH: {epoch}')\n",
        "  print(f'Loss: {mean(epoch_losses)}')\n",
        "  if epoch % 10 == 0:\n",
        "    PATH = r'/content/drive/MyDrive/Colab Notebooks/FibreAnalysis/SAMTraining'\n",
        "    filename = os.path.join(PATH, f'SAMModel_epoch_{epoch}.pt')\n",
        "    torch.save(sam_model.state_dict(), filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  epoch = 20\n",
        "  if epoch % 10 == 0:\n",
        "    PATH = r'/content/drive/MyDrive/Colab Notebooks/FibreAnalysis/SAMTraining'\n",
        "    filename = os.path.join(PATH, f'SAMModel_epoch_test_{epoch}.pt')\n",
        "    torch.save(sam_model.state_dict(), filename)"
      ],
      "metadata": {
        "id": "6XK9ibugxTnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = r'/content/drive/MyDrive/Colab Notebooks/FibreAnalysis/SAMModel1'\n",
        "torch.save(sam_model.state_dict(), PATH)"
      ],
      "metadata": {
        "id": "0tBNwc8kv85R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnfuwzFebkLy"
      },
      "source": [
        "## Display losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bLJ5Gc_bkLy"
      },
      "outputs": [],
      "source": [
        "mean_losses = [mean(x) for x in losses]\n",
        "# mean_losses\n",
        "\n",
        "plt.plot(list(range(len(mean_losses))), mean_losses)\n",
        "plt.title('Mean epoch loss')\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3tsIA2YbkLy"
      },
      "source": [
        "### Display test image with the predicted masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQMes0jzbkLy"
      },
      "outputs": [],
      "source": [
        "\n",
        "img_files = os.listdir(os.path.join(current_directory, 'train/val')) # Get list of files in the directory\n",
        "for imageFile in random.sample(img_files, 1):  # Take one example\n",
        "\n",
        "   image = cv2.imread(imageFile)\n",
        "   mask_generator = SamAutomaticMaskGenerator(sam_model)\n",
        "   masks = mask_generator.generate(image)\n",
        "\n",
        "   plt.figure(figsize=(20,20))\n",
        "   plt.imshow(image)\n",
        "   show_anns(masks)\n",
        "   plt.axis('off')\n",
        "   plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}