{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjHGvgwfdt5p",
        "outputId": "066cf489-53e0-4f95-abb4-29a2972866ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "current_directory = '/content/drive/MyDrive/Colab Notebooks/FibreAnalysis/Data'\n",
        "\n",
        "from google.colab.patches import cv2_imshow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os"
      ],
      "metadata": {
        "id": "cnEbRLnZeBS6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-BROpCEdt5q",
        "outputId": "3cc955c1-0491-4924-8937-80769f76c08c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'detectron2'...\n",
            "remote: Enumerating objects: 15180, done.\u001b[K\n",
            "remote: Counting objects: 100% (201/201), done.\u001b[K\n",
            "remote: Compressing objects: 100% (125/125), done.\u001b[K\n",
            "remote: Total 15180 (delta 110), reused 138 (delta 76), pack-reused 14979\u001b[K\n",
            "Receiving objects: 100% (15180/15180), 6.22 MiB | 22.98 MiB/s, done.\n",
            "Resolving deltas: 100% (10977/10977), done.\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (2.0.6)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (2.3.0)\n",
            "Collecting yacs>=0.1.8\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (0.9.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (2.2.1)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.10/dist-packages (4.65.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.12.3)\n",
            "Collecting fvcore<0.1.6,>=0.1.5\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting iopath<0.1.10,>=0.1.7\n",
            "  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
            "Collecting omegaconf>=2.1\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hydra-core>=1.1\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting black\n",
            "  Downloading black-23.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (23.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.41.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.22.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from yacs>=0.1.8) (6.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.56.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.4.4)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.27.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (67.7.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.3.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.41.0)\n",
            "Collecting portalocker (from iopath<0.1.10,>=0.1.7)\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.1)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black) (8.1.6)\n",
            "Collecting mypy-extensions>=0.4.3 (from black)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting pathspec>=0.9.0 (from black)\n",
            "  Downloading pathspec-0.11.2-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black) (3.9.1)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black) (2.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard) (3.2.2)\n",
            "Building wheels for collected packages: fvcore, antlr4-python3-runtime\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61406 sha256=0025541b565f23fd21919e7bdfd859ad5d52fedd2d6666943091cbc974af6cf7\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144552 sha256=a75bb8737a95da33886c7cdc27dbb5a3f8749c2db71b6c8cc18b5886620b7d5e\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built fvcore antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, yacs, portalocker, pathspec, omegaconf, mypy-extensions, iopath, hydra-core, black, fvcore\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 black-23.7.0 fvcore-0.1.5.post20221221 hydra-core-1.3.2 iopath-0.1.9 mypy-extensions-1.0.0 omegaconf-2.3.0 pathspec-0.11.2 portalocker-2.7.0 yacs-0.1.8\n"
          ]
        }
      ],
      "source": [
        "import sys, os, distutils.core\n",
        "# Note: This is a faster way to install detectron2 in Colab, but it does not include all functionalities (e.g. compiled operators).\n",
        "# See https://detectron2.readthedocs.io/tutorials/install.html for full installation instructions\n",
        "!git clone 'https://github.com/facebookresearch/detectron2'\n",
        "dist = distutils.core.run_setup(\"./detectron2/setup.py\")\n",
        "!python -m pip install {' '.join([f\"'{x}'\" for x in dist.install_requires])}\n",
        "sys.path.insert(0, os.path.abspath('./detectron2'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRE7drmgdt5q",
        "outputId": "325cd5d6-201d-497f-9530-772722cbab80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.0.1+cu118\n",
            "Torchvision version: 0.15.2+cu118\n",
            "CUDA is available: True\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.7.0.72)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.22.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.41.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Collecting git+https://github.com/facebookresearch/segment-anything.git\n",
            "  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-req-build-njjrgulf\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-req-build-njjrgulf\n",
            "  Resolved https://github.com/facebookresearch/segment-anything.git to commit 6fdee8f2727f4506cfbbe553e23b895e27956588\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: segment-anything\n",
            "  Building wheel for segment-anything (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segment-anything: filename=segment_anything-1.0-py3-none-any.whl size=36588 sha256=6e04023054ff6f8bfeb5fc7ec8b91c6a7dbc2b9c6675b89d6edee3ef7dc386cf\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-yv2y4bks/wheels/10/cf/59/9ccb2f0a1bcc81d4fbd0e501680b5d088d690c6cfbc02dc99d\n",
            "Successfully built segment-anything\n",
            "Installing collected packages: segment-anything\n",
            "Successfully installed segment-anything-1.0\n"
          ]
        }
      ],
      "source": [
        "using_colab = True\n",
        "if using_colab:\n",
        "    import torch\n",
        "    import torchvision\n",
        "    print(\"PyTorch version:\", torch.__version__)\n",
        "    print(\"Torchvision version:\", torchvision.__version__)\n",
        "    print(\"CUDA is available:\", torch.cuda.is_available())\n",
        "    import sys\n",
        "    !{sys.executable} -m pip install opencv-python matplotlib\n",
        "    !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9_cpvy9ldt5q",
        "outputId": "f46b49b0-1466-47eb-da3e-65f54cedc1e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/luca-medeiros/lang-segment-anything.git\n",
            "  Cloning https://github.com/luca-medeiros/lang-segment-anything.git to /tmp/pip-req-build-s0xj_qul\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/luca-medeiros/lang-segment-anything.git /tmp/pip-req-build-s0xj_qul\n",
            "  Resolved https://github.com/luca-medeiros/lang-segment-anything.git to commit 2ebcd001b0ed9dac9645ed54c35def2bba3ddbd2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting groundingdino@ git+https://github.com/IDEA-Research/GroundingDINO.git (from lang-sam==0.1.0)\n",
            "  Cloning https://github.com/IDEA-Research/GroundingDINO.git to /tmp/pip-install-un_ar4bc/groundingdino_fa4e708be1f046eb92a2e525d6118d27\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/IDEA-Research/GroundingDINO.git /tmp/pip-install-un_ar4bc/groundingdino_fa4e708be1f046eb92a2e525d6118d27\n",
            "  Resolved https://github.com/IDEA-Research/GroundingDINO.git to commit 60d796825e1266e56f7e4e9e00e88de662b67bd3\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting segment-anything@ git+https://github.com/facebookresearch/segment-anything.git (from lang-sam==0.1.0)\n",
            "  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-install-un_ar4bc/segment-anything_105c9eaa1c1146318f630246c13fed70\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-install-un_ar4bc/segment-anything_105c9eaa1c1146318f630246c13fed70\n",
            "  Resolved https://github.com/facebookresearch/segment-anything.git to commit 6fdee8f2727f4506cfbbe553e23b895e27956588\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting Pillow==9.3.0 (from lang-sam==0.1.0)\n",
            "  Downloading Pillow-9.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gradio<4.0.0,>=3.24.1 (from lang-sam==0.1.0)\n",
            "  Downloading gradio-3.39.0-py3-none-any.whl (19.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.9/19.9 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<0.14.0,>=0.13.4 (from lang-sam==0.1.0)\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lightning<3.0.0,>=2.0.1 (from lang-sam==0.1.0)\n",
            "  Downloading lightning-2.0.6-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy<2.0.0,>=1.24.2 (from lang-sam==0.1.0)\n",
            "  Downloading numpy-1.25.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv_python<5.0.0.0,>=4.7.0.72 in /usr/local/lib/python3.10/dist-packages (from lang-sam==0.1.0) (4.7.0.72)\n",
            "Collecting transformers<5.0.0,>=4.27.4 (from lang-sam==0.1.0)\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio<4.0.0,>=3.24.1->lang-sam==0.1.0)\n",
            "  Downloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: aiohttp~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (3.8.5)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (4.2.2)\n",
            "Collecting fastapi (from gradio<4.0.0,>=3.24.1->lang-sam==0.1.0)\n",
            "  Downloading fastapi-0.100.1-py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio<4.0.0,>=3.24.1->lang-sam==0.1.0)\n",
            "  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client>=0.3.0 (from gradio<4.0.0,>=3.24.1->lang-sam==0.1.0)\n",
            "  Downloading gradio_client-0.3.0-py3-none-any.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.2/294.2 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx (from gradio<4.0.0,>=3.24.1->lang-sam==0.1.0)\n",
            "  Downloading httpx-0.24.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of gradio to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting gradio<4.0.0,>=3.24.1 (from lang-sam==0.1.0)\n",
            "  Downloading gradio-3.38.0-py3-none-any.whl (19.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading gradio-3.37.0-py3-none-any.whl (19.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading gradio-3.36.1-py3-none-any.whl (19.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading gradio-3.36.0-py3-none-any.whl (19.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading gradio-3.35.2-py3-none-any.whl (19.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading gradio-3.35.1-py3-none-any.whl (19.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading gradio-3.35.0-py3-none-any.whl (19.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of gradio to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading gradio-3.34.0-py3-none-any.whl (20.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading gradio-3.33.1-py3-none-any.whl (20.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading gradio-3.33.0-py3-none-any.whl (20.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading gradio-3.32.0-py3-none-any.whl (19.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.9/19.9 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (3.1.2)\n",
            "Requirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (3.0.0)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.10/dist-packages (from gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (2.1.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (3.7.1)\n",
            "Collecting mdit-py-plugins<=0.3.3 (from gradio<4.0.0,>=3.24.1->lang-sam==0.1.0)\n",
            "  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson (from gradio<4.0.0,>=3.24.1->lang-sam==0.1.0)\n",
            "  Downloading orjson-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (1.5.3)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (1.10.12)\n",
            "Collecting pydub (from gradio<4.0.0,>=3.24.1->lang-sam==0.1.0)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: pygments>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (2.14.0)\n",
            "Collecting python-multipart (from gradio<4.0.0,>=3.24.1->lang-sam==0.1.0)\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (2.27.1)\n",
            "Collecting semantic-version (from gradio<4.0.0,>=3.24.1->lang-sam==0.1.0)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (4.7.1)\n",
            "Collecting uvicorn>=0.14.0 (from gradio<4.0.0,>=3.24.1->lang-sam==0.1.0)\n",
            "  Downloading uvicorn-0.23.1-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.0 (from gradio<4.0.0,>=3.24.1->lang-sam==0.1.0)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.14.0,>=0.13.4->lang-sam==0.1.0) (3.12.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.14.0,>=0.13.4->lang-sam==0.1.0) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.14.0,>=0.13.4->lang-sam==0.1.0) (23.1)\n",
            "Collecting arrow<3.0,>=1.2.0 (from lightning<3.0.0,>=2.0.1->lang-sam==0.1.0)\n",
            "  Downloading arrow-1.2.3-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting backoff<4.0,>=2.2.1 (from lightning<3.0.0,>=2.0.1->lang-sam==0.1.0)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: beautifulsoup4<6.0,>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from lightning<3.0.0,>=2.0.1->lang-sam==0.1.0) (4.11.2)\n",
            "Requirement already satisfied: click<10.0 in /usr/local/lib/python3.10/dist-packages (from lightning<3.0.0,>=2.0.1->lang-sam==0.1.0) (8.1.6)\n",
            "Collecting croniter<1.5.0,>=1.3.0 (from lightning<3.0.0,>=2.0.1->lang-sam==0.1.0)\n",
            "  Downloading croniter-1.4.1-py2.py3-none-any.whl (19 kB)\n",
            "Collecting dateutils<2.0 (from lightning<3.0.0,>=2.0.1->lang-sam==0.1.0)\n",
            "  Downloading dateutils-0.6.12-py2.py3-none-any.whl (5.7 kB)\n",
            "Collecting deepdiff<8.0,>=5.7.0 (from lightning<3.0.0,>=2.0.1->lang-sam==0.1.0)\n",
            "  Downloading deepdiff-6.3.1-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.7/70.7 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec<2025.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from lightning<3.0.0,>=2.0.1->lang-sam==0.1.0) (2023.6.0)\n",
            "Collecting inquirer<5.0,>=2.10.0 (from lightning<3.0.0,>=2.0.1->lang-sam==0.1.0)\n",
            "  Downloading inquirer-3.1.3-py3-none-any.whl (18 kB)\n",
            "Collecting lightning-cloud>=0.5.37 (from lightning<3.0.0,>=2.0.1->lang-sam==0.1.0)\n",
            "  Downloading lightning_cloud-0.5.37-py3-none-any.whl (596 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m596.7/596.7 kB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lightning-utilities<2.0,>=0.7.0 (from lightning<3.0.0,>=2.0.1->lang-sam==0.1.0)\n",
            "  Downloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: psutil<7.0 in /usr/local/lib/python3.10/dist-packages (from lightning<3.0.0,>=2.0.1->lang-sam==0.1.0) (5.9.5)\n",
            "Requirement already satisfied: rich<15.0,>=12.3.0 in /usr/local/lib/python3.10/dist-packages (from lightning<3.0.0,>=2.0.1->lang-sam==0.1.0) (13.4.2)\n",
            "Collecting starlette (from lightning<3.0.0,>=2.0.1->lang-sam==0.1.0)\n",
            "  Downloading starlette-0.31.0-py3-none-any.whl (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.8/69.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starsessions<2.0,>=1.2.1 (from lightning<3.0.0,>=2.0.1->lang-sam==0.1.0)\n",
            "  Downloading starsessions-1.3.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: torch<4.0,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from lightning<3.0.0,>=2.0.1->lang-sam==0.1.0) (2.0.1+cu118)\n",
            "Collecting torchmetrics<2.0,>=0.7.0 (from lightning<3.0.0,>=2.0.1->lang-sam==0.1.0)\n",
            "  Downloading torchmetrics-1.0.1-py3-none-any.whl (729 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m729.2/729.2 kB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: traitlets<7.0,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from lightning<3.0.0,>=2.0.1->lang-sam==0.1.0) (5.7.1)\n",
            "Requirement already satisfied: urllib3<4.0 in /usr/local/lib/python3.10/dist-packages (from lightning<3.0.0,>=2.0.1->lang-sam==0.1.0) (1.26.16)\n",
            "Requirement already satisfied: websocket-client<3.0 in /usr/local/lib/python3.10/dist-packages (from lightning<3.0.0,>=2.0.1->lang-sam==0.1.0) (1.6.1)\n",
            "Collecting pytorch-lightning (from lightning<3.0.0,>=2.0.1->lang-sam==0.1.0)\n",
            "  Downloading pytorch_lightning-2.0.6-py3-none-any.whl (722 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m722.8/722.8 kB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of transformers to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting transformers<5.0.0,>=4.27.4 (from lang-sam==0.1.0)\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.30.1-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m114.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.30.0-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m113.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m105.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.29.1-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m118.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.29.0-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m103.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.27.4->lang-sam==0.1.0) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<5.0.0,>=4.27.4->lang-sam==0.1.0)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m113.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from groundingdino@ git+https://github.com/IDEA-Research/GroundingDINO.git->lang-sam==0.1.0) (0.15.2+cu118)\n",
            "Collecting addict (from groundingdino@ git+https://github.com/IDEA-Research/GroundingDINO.git->lang-sam==0.1.0)\n",
            "  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Collecting yapf (from groundingdino@ git+https://github.com/IDEA-Research/GroundingDINO.git->lang-sam==0.1.0)\n",
            "  Downloading yapf-0.40.1-py3-none-any.whl (250 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.3/250.3 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting timm (from groundingdino@ git+https://github.com/IDEA-Research/GroundingDINO.git->lang-sam==0.1.0)\n",
            "  Downloading timm-0.9.2-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting supervision==0.6.0 (from groundingdino@ git+https://github.com/IDEA-Research/GroundingDINO.git->lang-sam==0.1.0)\n",
            "  Downloading supervision-0.6.0-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (from groundingdino@ git+https://github.com/IDEA-Research/GroundingDINO.git->lang-sam==0.1.0) (2.0.6)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (4.3.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (0.12.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from arrow<3.0,>=1.2.0->lightning<3.0.0,>=2.0.1->lang-sam==0.1.0) (2.8.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<6.0,>=4.8.0->lightning<3.0.0,>=2.0.1->lang-sam==0.1.0) (2.4.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from dateutils<2.0->lightning<3.0.0,>=2.0.1->lang-sam==0.1.0) (2022.7.1)\n",
            "Collecting ordered-set<4.2.0,>=4.0.2 (from deepdiff<8.0,>=5.7.0->lightning<3.0.0,>=2.0.1->lang-sam==0.1.0)\n",
            "  Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
            "Collecting starlette (from lightning<3.0.0,>=2.0.1->lang-sam==0.1.0)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (1.3.1)\n",
            "Collecting blessed>=1.19.0 (from inquirer<5.0,>=2.10.0->lightning<3.0.0,>=2.0.1->lang-sam==0.1.0)\n",
            "  Downloading blessed-1.20.0-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-editor>=1.0.4 (from inquirer<5.0,>=2.10.0->lightning<3.0.0,>=2.0.1->lang-sam==0.1.0)\n",
            "  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n",
            "Collecting readchar>=3.0.6 (from inquirer<5.0,>=2.10.0->lightning<3.0.0,>=2.0.1->lang-sam==0.1.0)\n",
            "  Downloading readchar-4.0.5-py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: pyjwt in /usr/lib/python3/dist-packages (from lightning-cloud>=0.5.37->lightning<3.0.0,>=2.0.1->lang-sam==0.1.0) (2.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from lightning-cloud>=0.5.37->lightning<3.0.0,>=2.0.1->lang-sam==0.1.0) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (0.1.2)\n",
            "Requirement already satisfied: linkify-it-py<3,>=1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (2.0.2)\n",
            "INFO: pip is looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting mdit-py-plugins<=0.3.3 (from gradio<4.0.0,>=3.24.1->lang-sam==0.1.0)\n",
            "  Downloading mdit_py_plugins-0.3.2-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.3.1-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.3.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.8-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.7-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.6-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.5-py3-none-any.whl (39 kB)\n",
            "INFO: pip is looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading mdit_py_plugins-0.2.4-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.3-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.2-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.1-py3-none-any.whl (38 kB)\n",
            "  Downloading mdit_py_plugins-0.2.0-py3-none-any.whl (38 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading mdit_py_plugins-0.1.0-py3-none-any.whl (37 kB)\n",
            "Collecting markdown-it-py[linkify]>=2.0.0 (from gradio<4.0.0,>=3.24.1->lang-sam==0.1.0)\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (2023.7.22)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (3.4)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette->lightning<3.0.0,>=2.0.1->lang-sam==0.1.0) (3.7.1)\n",
            "Requirement already satisfied: itsdangerous<3.0.0,>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from starsessions<2.0,>=1.2.1->lightning<3.0.0,>=2.0.1->lang-sam==0.1.0) (2.1.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.11.0->lightning<3.0.0,>=2.0.1->lang-sam==0.1.0) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.11.0->lightning<3.0.0,>=2.0.1->lang-sam==0.1.0) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.11.0->lightning<3.0.0,>=2.0.1->lang-sam==0.1.0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch<4.0,>=1.11.0->lightning<3.0.0,>=2.0.1->lang-sam==0.1.0) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch<4.0,>=1.11.0->lightning<3.0.0,>=2.0.1->lang-sam==0.1.0) (16.0.6)\n",
            "Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpcore<0.18.0,>=0.15.0 (from httpx->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0)\n",
            "  Downloading httpcore-0.17.3-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (4.41.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (3.1.0)\n",
            "Collecting safetensors (from timm->groundingdino@ git+https://github.com/IDEA-Research/GroundingDINO.git->lang-sam==0.1.0)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting importlib-metadata>=6.6.0 (from yapf->groundingdino@ git+https://github.com/IDEA-Research/GroundingDINO.git->lang-sam==0.1.0)\n",
            "  Downloading importlib_metadata-6.8.0-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.10/dist-packages (from yapf->groundingdino@ git+https://github.com/IDEA-Research/GroundingDINO.git->lang-sam==0.1.0) (3.9.1)\n",
            "Requirement already satisfied: tomli>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from yapf->groundingdino@ git+https://github.com/IDEA-Research/GroundingDINO.git->lang-sam==0.1.0) (2.0.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette->lightning<3.0.0,>=2.0.1->lang-sam==0.1.0) (1.1.2)\n",
            "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from blessed>=1.19.0->inquirer<5.0,>=2.10.0->lightning<3.0.0,>=2.0.1->lang-sam==0.1.0) (0.2.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.6.0->yapf->groundingdino@ git+https://github.com/IDEA-Research/GroundingDINO.git->lang-sam==0.1.0) (3.16.2)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (0.19.3)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.10/dist-packages (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio<4.0.0,>=3.24.1->lang-sam==0.1.0) (1.0.2)\n",
            "Requirement already satisfied: setuptools>=41.0 in /usr/local/lib/python3.10/dist-packages (from readchar>=3.0.6->inquirer<5.0,>=2.10.0->lightning<3.0.0,>=2.0.1->lang-sam==0.1.0) (67.7.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<4.0,>=1.11.0->lightning<3.0.0,>=2.0.1->lang-sam==0.1.0) (1.3.0)\n",
            "Building wheels for collected packages: lang-sam, groundingdino, ffmpy\n",
            "  Building wheel for lang-sam (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lang-sam: filename=lang_sam-0.1.0-py3-none-any.whl size=9476 sha256=262b0c26c39577d9b16eb65da48903cde567b0ac63f507f247bec2840999b54f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-pp1lt2uu/wheels/c7/1d/cd/6f34f8ca4fbc01b7a1f522cf9c4654cbe4fcd4268069d6e739\n",
            "  Building wheel for groundingdino (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for groundingdino: filename=groundingdino-0.1.0-cp310-cp310-linux_x86_64.whl size=2868820 sha256=e867f56da49ea1af54336bc2f3a83fdf30dbe3752c3bdc22b319a5edfe85e7de\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-pp1lt2uu/wheels/6b/06/d7/b57f601a4df56af41d262a5b1b496359b13c323bf5ef0434b2\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=7a0d23e4555da522d516ea78fece2e51d31b65fc09379c06227f894a69e2c70c\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\n",
            "Successfully built lang-sam groundingdino ffmpy\n",
            "Installing collected packages: tokenizers, safetensors, python-editor, pydub, ffmpy, addict, websockets, semantic-version, readchar, python-multipart, Pillow, orjson, ordered-set, numpy, markdown-it-py, lightning-utilities, importlib-metadata, h11, blessed, backoff, aiofiles, yapf, uvicorn, starlette, mdit-py-plugins, inquirer, huggingface-hub, httpcore, deepdiff, dateutils, croniter, arrow, transformers, starsessions, httpx, fastapi, supervision, lightning-cloud, gradio-client, gradio, torchmetrics, timm, pytorch-lightning, lightning, groundingdino, lang-sam\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 9.4.0\n",
            "    Uninstalling Pillow-9.4.0:\n",
            "      Successfully uninstalled Pillow-9.4.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.4\n",
            "    Uninstalling numpy-1.22.4:\n",
            "      Successfully uninstalled numpy-1.22.4\n",
            "  Attempting uninstall: markdown-it-py\n",
            "    Found existing installation: markdown-it-py 3.0.0\n",
            "    Uninstalling markdown-it-py-3.0.0:\n",
            "      Successfully uninstalled markdown-it-py-3.0.0\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 4.6.4\n",
            "    Uninstalling importlib-metadata-4.6.4:\n",
            "      Successfully uninstalled importlib-metadata-4.6.4\n",
            "  Attempting uninstall: mdit-py-plugins\n",
            "    Found existing installation: mdit-py-plugins 0.4.0\n",
            "    Uninstalling mdit-py-plugins-0.4.0:\n",
            "      Successfully uninstalled mdit-py-plugins-0.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.25.1 which is incompatible.\n",
            "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.25.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Pillow-9.3.0 addict-2.4.0 aiofiles-23.1.0 arrow-1.2.3 backoff-2.2.1 blessed-1.20.0 croniter-1.4.1 dateutils-0.6.12 deepdiff-6.3.1 fastapi-0.100.1 ffmpy-0.3.1 gradio-3.32.0 gradio-client-0.3.0 groundingdino-0.1.0 h11-0.14.0 httpcore-0.17.3 httpx-0.24.1 huggingface-hub-0.13.4 importlib-metadata-6.8.0 inquirer-3.1.3 lang-sam-0.1.0 lightning-2.0.6 lightning-cloud-0.5.37 lightning-utilities-0.9.0 markdown-it-py-2.2.0 mdit-py-plugins-0.3.3 numpy-1.25.1 ordered-set-4.1.0 orjson-3.9.2 pydub-0.25.1 python-editor-1.0.4 python-multipart-0.0.6 pytorch-lightning-2.0.6 readchar-4.0.5 safetensors-0.3.1 semantic-version-2.10.0 starlette-0.27.0 starsessions-1.3.0 supervision-0.6.0 timm-0.9.2 tokenizers-0.13.3 torchmetrics-1.0.1 transformers-4.29.0 uvicorn-0.23.1 websockets-11.0.3 yapf-0.40.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "%pip install -U git+https://github.com/luca-medeiros/lang-segment-anything.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcKvEm4Vdt5r",
        "outputId": "8b577d95-92c1-4236-f17c-539080e03025"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.0.143-py3-none-any.whl (604 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m605.0/605.0 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.7.0.72)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.27.1)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.10.1)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.15.2+cu118)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.65.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.5.3)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.12.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (4.41.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (1.25.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (23.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2022.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->ultralytics) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->ultralytics) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->ultralytics) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->ultralytics) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->ultralytics) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->ultralytics) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7.0->ultralytics) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7.0->ultralytics) (16.0.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7.0->ultralytics) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7.0->ultralytics) (1.3.0)\n",
            "Installing collected packages: ultralytics\n",
            "Successfully installed ultralytics-8.0.143\n"
          ]
        }
      ],
      "source": [
        "# Install Ultralytics package\n",
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ue-m5sP6dt5r"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "#from scipy.stats import kruskal\n",
        "#import scikit_posthocs as sp\n",
        "import detectron2\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "from detectron2.structures import BoxMode\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.utils.visualizer import ColorMode\n",
        "import matplotlib.pyplot as plt\n",
        "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
        "from lang_sam import LangSAM    #pip install -U git+https://github.com/luca-medeiros/lang-segment-anything.git\n",
        "from PIL import Image\n",
        "from lang_sam.utils import draw_image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "J7h1NrS_dt5r"
      },
      "outputs": [],
      "source": [
        "\n",
        "def GetYOLOResult_Contour(results):\n",
        "\n",
        "    # Create a list to store dictionaries with area and count for each result\n",
        "    result_data = []\n",
        "\n",
        "    for result in results:\n",
        "        areas = []\n",
        "        image_shape = result.orig_shape\n",
        "\n",
        "        overall_mask = np.zeros((result.orig_shape[0], result.orig_shape[1]), dtype=np.uint8)\n",
        "       # print(result)\n",
        "        for j, mask in enumerate(result.masks.data):\n",
        "        #   # Move the mask tensor to CPU if it's on a CUDA device\n",
        "            mask_np = mask.detach().cpu().numpy() if isinstance(mask, torch.Tensor) else mask\n",
        "\n",
        "        #   # Convert the mask to uint8 and resize it to match the image size\n",
        "#            mask_np = cv2.resize((mask_np.astype( np.uint8) * 255), (result.orig_shape[1], result.orig_shape[0]))\n",
        "\n",
        "            # Resize to original image size, Using INTER_NEAREST ensures resized mask remains binary with values 0 and 1, preserving its original nature.\n",
        "            mask_np_resized = cv2.resize(mask_np, (result.orig_shape[1], result.orig_shape[0]), interpolation=cv2.INTER_NEAREST).astype(np.uint8)\n",
        "\n",
        "            contours, _ = cv2.findContours(mask_np_resized, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    # Initialize the total area for this mask\n",
        "            total_area = 0.0\n",
        "\n",
        "            # Iterate through each contour and calculate its area\n",
        "            for contour in contours:\n",
        "                area = cv2.contourArea(contour)\n",
        "                total_area += area\n",
        "\n",
        "            #area_covered = np.sum(mask_np_resized)\n",
        "            areas.append(total_area)\n",
        "\n",
        "            # Combine the current mask with the overall mask using logical OR operation\n",
        "            overall_mask = cv2.bitwise_or(overall_mask, mask_np_resized)\n",
        "\n",
        "        result_data.append({'area': sum(areas), 'count': len(areas), 'overallMask': overall_mask})\n",
        "\n",
        "    return result_data  # get count of fibres and area covered\n",
        "\n",
        "\n",
        "def GetYOLOResult(results):\n",
        "\n",
        "    # Create a list to store dictionaries with area and count for each result\n",
        "    result_data = []\n",
        "\n",
        "    for result in results:\n",
        "        areas = []\n",
        "        image_shape = result.orig_shape\n",
        "        overall_mask = np.zeros((result.orig_shape[0], result.orig_shape[1]), dtype=np.uint8)\n",
        "        for j, mask in enumerate(result.masks.data):\n",
        "        #   # Move the mask tensor to CPU if it's on a CUDA device\n",
        "            mask_np = mask.detach().cpu().numpy() if isinstance(mask, torch.Tensor) else mask\n",
        "\n",
        "        #   # Convert the mask to uint8 and resize it to match the image size\n",
        "#            mask_np = cv2.resize((mask_np.astype( np.uint8) * 255), (result.orig_shape[1], result.orig_shape[0]))\n",
        "\n",
        "            # Resize to original image size, Using INTER_NEAREST ensures resized mask remains binary with values 0 and 1, preserving its original nature.\n",
        "            mask_np_resized = cv2.resize(mask_np, (result.orig_shape[1], result.orig_shape[0]), interpolation=cv2.INTER_NEAREST).astype(np.uint8)\n",
        "\n",
        "            area_covered = np.sum(mask_np_resized)\n",
        "            areas.append(area_covered)\n",
        "\n",
        "            # Combine the current mask with the overall mask using logical OR operation\n",
        "            overall_mask = cv2.bitwise_or(overall_mask, mask_np_resized)\n",
        "\n",
        "        result_data.append({'area': sum(areas), 'count': len(areas), 'overallMask': overall_mask})\n",
        "\n",
        "    return result_data  # get count of fibres and area covered\n",
        "\n",
        "\n",
        "\n",
        "def getMaskContours(mask):\n",
        "\n",
        " # Create a list to store dictionaries with area and count for each result\n",
        "    result_data = []\n",
        "\n",
        "    areas = []\n",
        "\n",
        "    # Calculate the contour of the mask using cv2.findContours (only external contours)\n",
        "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "        # Initialize the total area for this mask\n",
        "    total_area = 0.0\n",
        "\n",
        "        # Iterate through each contour and calculate its area\n",
        "    for contour in contours:\n",
        "        area = cv2.contourArea(contour)\n",
        "        total_area += area\n",
        "\n",
        "    areas.append(total_area)\n",
        "\n",
        "    result_data.append({'area': sum(areas), 'count': len(contours)})\n",
        "\n",
        "    return result_data  # get count of fibres and area covered\n",
        "\n",
        "\n",
        "\n",
        "def GetDetectronResult(results):\n",
        "\n",
        "    # Create a list to store dictionaries with area and count for each result\n",
        "    result_data = []\n",
        "\n",
        "    # Access the predicted masks\n",
        "    predicted_masks = results[\"instances\"].pred_masks\n",
        "    predicted_masks_np = predicted_masks.cpu().numpy().astype(np.uint8)\n",
        "\n",
        "    image_size = results[\"instances\"].image_size\n",
        "    height, width = image_size\n",
        "\n",
        "    overall_mask = np.zeros((height, width), dtype=np.uint8)\n",
        "    areas = []\n",
        "    for i, mask in enumerate(predicted_masks_np):\n",
        "\n",
        "    # Calculate the contour of the mask using cv2.findContours (only external contours)\n",
        "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "        # Initialize the total area for this mask\n",
        "        total_area = 0.0\n",
        "\n",
        "        # Iterate through each contour and calculate its area\n",
        "        for contour in contours:\n",
        "            area = cv2.contourArea(contour)\n",
        "            total_area += area\n",
        "\n",
        "        areas.append(total_area)\n",
        "\n",
        "        # Combine the current mask with the overall mask using logical OR operation\n",
        "        overall_mask = cv2.bitwise_or(overall_mask, mask)\n",
        "\n",
        "    result_data.append({'area': sum(areas), 'count': len(areas), 'overallMask': overall_mask})\n",
        "\n",
        "    return result_data  # get count of fibres and area covered\n",
        "\n",
        "\n",
        "def GetSAMresult(results):\n",
        "\n",
        "    # Create a list to store dictionaries with area and count for each result\n",
        "    result_data = []\n",
        "\n",
        "    width = results[0]['crop_box'][2]          # Cropping is the same for all the image and so represents the size of the image\n",
        "    height = results[0]['crop_box'][3]\n",
        "    overall_mask = np.zeros((height, width), dtype=np.uint8)\n",
        "    areas = []\n",
        "#    print('results', results)\n",
        "\n",
        "    for i in range(1, len(results)):   # Ignore the first finding because that always seemsto be the entire shape\n",
        "        results[i]['segmentation']\n",
        "        # Move the mask tensor to CPU if it's on a CUDA device\n",
        "        #mask_np = results[i]['segmentation'].detach().cpu().numpy() if isinstance(results[i]['segmentation'], torch.Tensor) else results[i]['segmentation']\n",
        "\n",
        "        mask_tensor = results[i]['segmentation']\n",
        "        if isinstance(mask_tensor, torch.Tensor):\n",
        "            mask_np = mask_tensor.detach().cpu().numpy()\n",
        "        else:\n",
        "            mask_np = mask_tensor\n",
        "#        overall_mask[mask_np] = 1\n",
        "        areas.append(results[i]['area'])\n",
        "\n",
        "        # Combine the current mask with the overall mask using logical OR operation\n",
        "        #overall_mask = cv2.bitwise_or(overall_mask, mask_np)\n",
        "        # Find the maximum value in mask_np\n",
        "        #max_value = np.max(mask_np)\n",
        "        #print(\"Maximum value in mask_np:\", max_value, 'loop', i)\n",
        "\n",
        "        overall_mask[mask_np] = 1\n",
        "    result_data.append({'area': sum(areas), 'count': len(results), 'overallMask': overall_mask})\n",
        "    return result_data  # get count of fibres and area covered\n",
        "\n",
        "\n",
        "\n",
        "def getSAMLang_Result(result):\n",
        "\n",
        "    result_data = []\n",
        "    mask_np = result.detach().cpu().numpy().astype(np.uint8) if isinstance(result, torch.Tensor) else result\n",
        "\n",
        "   # Access the predicted masks\n",
        "\n",
        "    image_size = mask_np.shape\n",
        "\n",
        "    height = image_size[1]\n",
        "    width = image_size[2]\n",
        "\n",
        "    overall_mask = np.zeros((height, width), dtype=np.uint8)\n",
        "    areas = []\n",
        "    for i, mask in enumerate(mask_np):\n",
        "\n",
        "    # Calculate the contour of the mask using cv2.findContours (only external contours)\n",
        "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "        # Initialize the total area for this mask\n",
        "        total_area = 0.0\n",
        "\n",
        "        # Iterate through each contour and calculate its area\n",
        "        for contour in contours:\n",
        "            area = cv2.contourArea(contour)\n",
        "            total_area += area\n",
        "\n",
        "        areas.append(total_area)\n",
        "\n",
        "        # Combine the current mask with the overall mask using logical OR operation\n",
        "        overall_mask = cv2.bitwise_or(overall_mask, mask)\n",
        "\n",
        "    result_data.append({'area': sum(areas), 'count': len(areas), 'overallMask': overall_mask})\n",
        "\n",
        "    return result_data  # get count of fibres and area covered\n",
        "\n",
        "def show_anns(anns):\n",
        "    # for use when displaying segment anything mask\n",
        "    if len(anns) == 0:\n",
        "        return\n",
        "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
        "    ax = plt.gca()\n",
        "    ax.set_autoscale_on(False)\n",
        "\n",
        "    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n",
        "    img[:,:,3] = 0\n",
        "    for ann in sorted_anns:\n",
        "        m = ann['segmentation']\n",
        "        color_mask = np.concatenate([np.random.random(3), [0.35]])\n",
        "        img[m] = color_mask\n",
        "    ax.imshow(img)\n",
        "\n",
        "def prep_mask_image(anns):\n",
        "    if len(anns) == 0:\n",
        "        return None\n",
        "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
        "    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n",
        "    img[:,:,3] = 0\n",
        "    for ann in sorted_anns:\n",
        "        m = ann['segmentation']\n",
        "        color_mask = np.concatenate([np.random.random(3), [0.35]])\n",
        "        img[m] = color_mask\n",
        "    img = (img * 255).astype(np.uint8)  # Convert to uint8 range (0-255)\n",
        "    return Image.fromarray(img)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def calculate_iou(groundTruth, predicted):\n",
        "\n",
        "   # print(' calc iou', np.max(predicted))\n",
        "\n",
        "    intersection = np.logical_and(groundTruth, predicted).sum()\n",
        "    union = np.logical_or(groundTruth, predicted).sum()\n",
        "    iou = intersection / union\n",
        "\n",
        "    # Next am curious as to how much of the predicted objects are contained within the mask objects\n",
        "    # so am checking how much of the intersenting is in the predicted. This is because I think the ground truth masks\n",
        "    # might be a bit bigger than the fibres so it might score lower on IOU but still be an accurate prediction.\n",
        "    int_predicted_intersection = intersection / predicted.sum()\n",
        "\n",
        "    dice_coefficient = 2 * np.sum(intersection) / (np.sum(groundTruth) + np.sum(predicted))\n",
        "    return iou , dice_coefficient, int_predicted_intersection\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQE4mX0kdt5s",
        "outputId": "77aa0fe2-6d13-4967-b035-63bf297a05c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded from /root/.cache/huggingface/hub/models--ShilongLiu--GroundingDINO/snapshots/a94c9b567a2a374598f05c584e96798a170c56fb/groundingdino_swinb_cogcoor.pth \n",
            " => _IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight'])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "   # train on the GPU or on the CPU, if a GPU is not available - CPU will be very slow\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device)\n",
        "# Source Data\n",
        "\n",
        "fibre_images = '/content/drive/MyDrive/Colab Notebooks/FibreAnalysis/Data/Prepared/Set/images/test'\n",
        "fibre_masks = '/content/drive/MyDrive/Colab Notebooks/FibreAnalysis/Data/Prepared/Set/masks/test'\n",
        "img_files = os.listdir(fibre_images) # Get list of files in the directory\n",
        "\n",
        "##########################################################################################\n",
        "# YOLO Model\n",
        "##########################################################################################\n",
        "# preTrainedYOLO = r'/content/drive/MyDrive/Colab Notebooks/FibreAnalysis/Models/YoloLTrain1.pt'\n",
        "# # Load a model\n",
        "# YOLOmodel = YOLO(preTrainedYOLO)  # pretrained YOLOv8n model\n",
        "\n",
        "# ##########################################################################################\n",
        "# # Detectron\n",
        "# ##########################################################################################\n",
        "# cfg = get_cfg()\n",
        "\n",
        "# #cfg.MODEL.DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "# cfg.MODEL.DEVICE = device.type\n",
        "# cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
        "# ###\n",
        "# ####### Change this\n",
        "# #cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\n",
        "# #cfg.MODEL.WEIGHTS = r'C:\\Users\\dezos\\Documents\\Fibres\\FibreAnalysis\\Step1_DataCreation\\Step4_Evaluate\\Detectron2_Trained_Model.pth'\n",
        "# cfg.MODEL.WEIGHTS = r'/content/drive/MyDrive/Colab Notebooks/FibreAnalysis/Models/Detectron2_Trained_Model.pth'\n",
        "# cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1 # Model was trained on one class so need to specify that here\n",
        "\n",
        "\n",
        "##########################################################################################\n",
        "# Segment Anything\n",
        "##########################################################################################\n",
        "\n",
        "sam_checkpoint = r\"/content/drive/MyDrive/Colab Notebooks/FibreAnalysis/Models/sam_vit_h_4b8939.pth\"\n",
        "model_type = \"vit_h\"\n",
        "\n",
        "device = device\n",
        "\n",
        "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
        "sam.to(device=device)\n",
        "\n",
        "sam_mask_generator = SamAutomaticMaskGenerator(sam)\n",
        "\n",
        "###########################################################################################\n",
        "# Language Segment anything\n",
        "###########################################################################################\n",
        "\n",
        "samL = LangSAM()\n",
        "\n",
        "##\n",
        "#cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
        "#Detect2Predictor = DefaultPredictor(cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uO3EiXgGdt5t",
        "outputId": "409ce179-1b5b-41b8-ccb5-4d6473d64e1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loop 0\n",
            "/content/drive/MyDrive/Colab Notebooks/FibreAnalysis/Data/Prepared/Set/images/test/image_2023-07-24_19-40-35-581733_1.png\n",
            "/content/drive/MyDrive/Colab Notebooks/FibreAnalysis/Data/Prepared/Set/masks/test/mask_2023-07-24_19-40-35-581733_1.png\n",
            "Maximum value in mask_np: True loop 1\n",
            "Maximum value in mask_np: True loop 2\n",
            "Maximum value in mask_np: True loop 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:874: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loop 1\n",
            "/content/drive/MyDrive/Colab Notebooks/FibreAnalysis/Data/Prepared/Set/images/test/image_2023-07-24_19-41-50-577683_2.png\n",
            "/content/drive/MyDrive/Colab Notebooks/FibreAnalysis/Data/Prepared/Set/masks/test/mask_2023-07-24_19-41-50-577683_2.png\n",
            "Maximum value in mask_np: True loop 1\n",
            "Maximum value in mask_np: True loop 2\n",
            "Maximum value in mask_np: True loop 3\n",
            "Maximum value in mask_np: True loop 4\n",
            "Maximum value in mask_np: True loop 5\n",
            "Maximum value in mask_np: True loop 6\n",
            "Maximum value in mask_np: True loop 7\n",
            "Maximum value in mask_np: True loop 8\n",
            "Maximum value in mask_np: True loop 9\n",
            "Maximum value in mask_np: True loop 10\n"
          ]
        }
      ],
      "source": [
        "dataHolder = []\n",
        "header_names = ['Image', 'YOLO_iou_value', 'YOLO_diceCoef' , 'YOLO_predIOU', 'Detect_iou_value', 'Detect_diceCoef', 'Detect_predIOU',\n",
        "                'SAM_iou_value', 'SAM_diceCoef', 'SAM_predIOU', 'SAMlang_iou_value', 'SAMlang_diceCoef', 'SAMlang_predIOU',\n",
        "                'SAM Area', 'SAM Count' , 'SAM Lang area', 'SAM Lang count', 'Ground Truth area', 'Ground Truth count' ]\n",
        "\n",
        "# Assuming 'counter' is defined elsewhere in your code\n",
        "\n",
        "path = r'/content/drive/MyDrive/Colab Notebooks/FibreAnalysis/EvalResults'\n",
        "\n",
        "counter = 0\n",
        "#for d in random.sample(img_files, 2):\n",
        "for i, d in enumerate(random.sample(img_files, 2)):\n",
        "    try:\n",
        "      print('loop', i)\n",
        "      fullPath = os.path.join(fibre_images, d)\n",
        "      mask_path = os.path.join(fibre_masks, f\"mask{d[5:]}\")  # Change filename from image_ to mask _ to retrieve mask\n",
        "      print(fullPath)  # Print File name\n",
        "      print(mask_path)  # Print File name\n",
        "      image = cv2.imread(fullPath)  # Load your input image\n",
        "      ground_truth_mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "      ground_mask_details = getMaskContours(ground_truth_mask)\n",
        "\n",
        "#      YOLOresult = GetYOLOResult_Contour(YOLOmodel(fullPath))   # get area and count for YOLO\n",
        "\n",
        "#      DetectronResult = GetDetectronResult(Detect2Predictor(image))\n",
        "      #print('detectron results', DetectronResult)\n",
        "      SAMmasks = sam_mask_generator.generate(image)\n",
        "      SAMresults = GetSAMresult(SAMmasks)\n",
        "\n",
        "      # Language\n",
        "\n",
        "      mask_image = prep_mask_image(SAMmasks)\n",
        "      grayscale_image = mask_image.convert(\"L\")\n",
        "      # Create a new RGB image with grayscale values in all channels\n",
        "      rgb_bw_image = Image.new(\"RGB\", grayscale_image.size)\n",
        "      rgb_bw_image.paste(grayscale_image)\n",
        "      text_prompt = \"line\"\n",
        "\n",
        "      SAMLmasks, SAMLboxes, SAMLlabels, SAMLlogits = samL.predict(rgb_bw_image, text_prompt, box_threshold=0.20, text_threshold=0.24)\n",
        "\n",
        "\n",
        "      SAMlang_results = getSAMLang_Result(SAMLmasks)\n",
        "      #Calculate IoU and Dice coefficient\n",
        "  #    YOLO_iou_value, YOLO_diceCoef , YOLO_predIOU = calculate_iou(ground_truth_mask, YOLOresult[-1][\"overallMask\"])\n",
        "  #    Detect_iou_value, Detect_diceCoef, Detect_predIOU = calculate_iou(ground_truth_mask, DetectronResult[-1][\"overallMask\"])\n",
        "\n",
        "      SAM_iou_value, SAM_diceCoef, SAM_predIOU = calculate_iou(ground_truth_mask, SAMresults[-1][\"overallMask\"])\n",
        "\n",
        "      SAMlang_iou_value, SAMlang_diceCoef, SAMlang_predIOU = calculate_iou(ground_truth_mask, SAMlang_results[-1][\"overallMask\"])\n",
        "\n",
        "\n",
        "      #Prepare rows for dataframe later. See this link as to why to create a list first (much faster)  https://stackoverflow.com/questions/10715965/create-a-pandas-dataframe-by-appending-one-row-at-a-time\n",
        "   #   dataHolder.append([d, YOLO_iou_value, YOLO_diceCoef , YOLO_predIOU, Detect_iou_value, Detect_diceCoef, Detect_predIOU,\n",
        "   #                         SAM_iou_value, SAM_diceCoef, SAM_predIOU, SAMlang_iou_value, SAMlang_diceCoef, SAMlang_predIOU ])\n",
        "\n",
        "      dataHolder.append([d, '0', '0' , '0', '0', '0', '0',\n",
        "                         SAM_iou_value, SAM_diceCoef, SAM_predIOU, SAMlang_iou_value, SAMlang_diceCoef, SAMlang_predIOU,\n",
        "                         SAMresults[-1][\"area\"], SAMresults[-1][\"count\"], SAMlang_results[-1][\"area\"], SAMlang_results[-1][\"count\"],\n",
        "                         ground_mask_details[-1][\"area\"], ground_mask_details[-1][\"count\"]])\n",
        "\n",
        "\n",
        "\n",
        "    # Increment the counter\n",
        "      counter += 1\n",
        "\n",
        "      # Check if the counter is a multiple of 10\n",
        "      if counter % 5 == 0:\n",
        "          # Create a DataFrame from the list\n",
        "          df = pd.DataFrame(dataHolder,  columns=header_names)\n",
        "\n",
        "          filename = f'IOUResults_{counter}.csv'\n",
        "          fullPath = os.path.join(path, filename)\n",
        "          df.to_csv(fullPath, index=False)\n",
        "\n",
        "    except Exception as e:\n",
        "    # Code to handle the error\n",
        "      print(\"An error occurred:\", e, fullPath )\n",
        "    # Additional error handling or fallback actions can be added here\n",
        "\n",
        "\n",
        "# After the loop ends, check if there are any remaining data in dataHolder\n",
        "# Save it to a CSV file if needed\n",
        "if dataHolder:\n",
        "    df = pd.DataFrame(dataHolder,  columns=header_names)\n",
        "    filename = f'IOUResults_{counter}.csv'\n",
        "    fullPath = os.path.join(path, filename)\n",
        "    df.to_csv(fullPath, index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "Mqmurc1IqICV"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the current GPU memory allocated by tensors in bytes\n",
        "allocated_memory = torch.cuda.memory_allocated()\n",
        "print(f\"Currently allocated GPU memory: {allocated_memory / 1024**2:.2f} MB\")\n",
        "\n",
        "# Print a summary of GPU memory usage and list of tensors\n",
        "summary = torch.cuda.memory_summary()\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAzklkk4sHrX",
        "outputId": "ba26ff90-0c74-4242-9dfe-08c9ddc041cd"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Currently allocated GPU memory: 6243.55 MB\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 33           |        cudaMalloc retries: 34        |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   6243 MiB |  12550 MiB |   9329 GiB |   9323 GiB |\n",
            "|       from large pool |   6068 MiB |  12362 MiB |   9262 GiB |   9256 GiB |\n",
            "|       from small pool |    174 MiB |    199 MiB |     66 GiB |     66 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   6243 MiB |  12550 MiB |   9329 GiB |   9323 GiB |\n",
            "|       from large pool |   6068 MiB |  12362 MiB |   9262 GiB |   9256 GiB |\n",
            "|       from small pool |    174 MiB |    199 MiB |     66 GiB |     66 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |   6163 MiB |  12412 MiB |   9325 GiB |   9319 GiB |\n",
            "|       from large pool |   5988 MiB |  12224 MiB |   9258 GiB |   9252 GiB |\n",
            "|       from small pool |    174 MiB |    199 MiB |     66 GiB |     66 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  12582 MiB |  13424 MiB |  21928 MiB |   9346 MiB |\n",
            "|       from large pool |  12382 MiB |  13220 MiB |  21412 MiB |   9030 MiB |\n",
            "|       from small pool |    200 MiB |    204 MiB |    516 MiB |    316 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   3242 MiB |   4535 MiB |   9270 GiB |   9266 GiB |\n",
            "|       from large pool |   3241 MiB |   4519 MiB |   9196 GiB |   9192 GiB |\n",
            "|       from small pool |      1 MiB |     17 MiB |     74 GiB |     74 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |    2794    |    3457    |     907 K  |     904 K  |\n",
            "|       from large pool |     530    |     699    |     134 K  |     134 K  |\n",
            "|       from small pool |    2264    |    2758    |     772 K  |     770 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |    2794    |    3457    |     907 K  |     904 K  |\n",
            "|       from large pool |     530    |     699    |     134 K  |     134 K  |\n",
            "|       from small pool |    2264    |    2758    |     772 K  |     770 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     244    |     351    |     515    |     271    |\n",
            "|       from large pool |     144    |     249    |     257    |     113    |\n",
            "|       from small pool |     100    |     102    |     258    |     158    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      68    |     132    |  388001    |  387933    |\n",
            "|       from large pool |      57    |     104    |   56783    |   56726    |\n",
            "|       from small pool |      11    |      35    |  331218    |  331207    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}